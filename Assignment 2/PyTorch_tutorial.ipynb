{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fXuuhJ2NPilg"
   },
   "source": [
    "# PyTorch Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ac8rTsQ6Rwiw"
   },
   "source": [
    "In this tutorial, we will go through the basics of PyTorch, training models on the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tAUaCTgDRGeS"
   },
   "source": [
    "Why PyTorch:\n",
    "\n",
    "- PyTorch \n",
    "\n",
    ">**+** Dynamic graphs run in a pythonic way, making it easier to understand what some piece of code is doing and to debug\n",
    "\n",
    ">**+** Offers a good amount of pretrained models and modules.\n",
    "\n",
    "> **-** It is still a young framework, so not as widely used as others\n",
    "\n",
    " \n",
    "\n",
    "- Keras\n",
    "\n",
    ">**\\+** Easiest framework to code simple models\n",
    "\n",
    ">**\\-** Lack of flexibility:  It is not made for models that use configurations that are not common\n",
    "\n",
    " \n",
    "\n",
    "- TensorFlow\n",
    "\n",
    ">**+** Framework usually used in industry, since it has TensorFlow serving, a tool to deploy models in production\n",
    "\n",
    ">**-** Static graph runs out of the python code, making it harder to debug\n",
    "\n",
    ">**-** Not well organized so far in terms of pre-coded layers and models (Tensorflow 2.0 should change that)\n",
    "\n",
    "\n",
    "**>** PyTorch ends up being a good option for research and for learning deep learning, since it is as flexible as TensorFlow, but easier to use for prototyping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rANxEkHeW6rw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3AWWcWjmOnBB"
   },
   "source": [
    "## Tensor basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uDClMW5TO4FW"
   },
   "source": [
    "PyTorch tensors are like numpy arrays, but that can be used with PyTorch modules and can be sent to GPUs. Several functions are similar to numpy functions, but several are not. Most tensor functions are listed in https://pytorch.org/docs/stable/torch.html#tensors or in https://pytorch.org/docs/stable/tensors.html\n",
    "\n",
    "Tensor shapes:\n",
    "- For linear layers: Batch Size X Channels\n",
    "- For 2d Convolutional Layer: Batch Size X Channels X Height X Width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "77v7wltOZLNT"
   },
   "source": [
    "#### Example of a few tensor operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yBW9DrUtXZ4Y"
   },
   "source": [
    "You can create tensors of different types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dkm8XPy1QIIG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(6, 3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CKqp6AB-XTgP"
   },
   "source": [
    "Use tensor.size() to get shape of tensor:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wmaPXCp_W-5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vXZTVkaRXLnL"
   },
   "source": [
    "Use tensor.view to reshape tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ya44efFdXBKg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(x.view(3,3,2).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0eB1QJ-ZXCdo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 3, 3, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(x, dim = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v9Gru0FhO5oH"
   },
   "source": [
    "#### Tensor devices and conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PgvZgk0YZavS"
   },
   "source": [
    "Tensors can be converted back and forth from numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gzSXf5yWZOzc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(np.array([[1 , 2],[3, 4]]))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sZrsxP3KZdG2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "print(x.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wnb0ubypZ1mE"
   },
   "source": [
    "Tensors can be sent back and forth from GPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7wZ3gZF-Z5j5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x = x.cuda()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z2IFUpCva5zM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "x = x.cpu()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jjKFFc_HZ7mF"
   },
   "source": [
    "Tensors should be on the same device as the model/module you run them on. So, to use GPU, you should have your input tensors stored on a GPU and your model stored on a GPU. For storing outputs of models or losses, it is better to transform them into numpy variables first. You can only convert to numpy tensors that are on a CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pLHCnloiaQNT"
   },
   "outputs": [],
   "source": [
    "# x = x.cuda().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v05ZOH5kOSK7"
   },
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o5o_GLCdOZFT"
   },
   "source": [
    "#### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qDydSlH3qVwD"
   },
   "source": [
    "The Dataset class in PyTorch is the one used to define data loading and transformations. For a custom dataset not offered by PyTorch, you will usually have to inherit from the dataset class to define how to load your data. More details here: https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset. Examples of this are provided in the Assignment 2 notebook. An object of this class can be indexed like python lists to get a specific dataset example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EPSOWuUJ7BBz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root = './', download=True, train = True, \n",
    "                           transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "]))\n",
    "\n",
    "#using test set as validation set for practical reasons in the tutorial\n",
    "# please do not do this to evaluate models\n",
    "val_dataset = torchvision.datasets.MNIST(root = './', download=True, train = False, \n",
    "                           transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2v3RBnsW7lbz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n",
      "tensor(5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "\n",
    "plt.imshow(train_dataset[0][0][0,:,:])\n",
    "print(train_dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nFCdlLXWOb7x"
   },
   "source": [
    "#### Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vwDTY5Xpplbm"
   },
   "source": [
    "PyTorch offers a few preprocessing functions for images (https://pytorch.org/docs/stable/torchvision/transforms.html). It contains both non-random transformations, for example resizing or normalizing images, and random transformations, for example random crops or random rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJUrWXs78Q92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7eff5ca69e48>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.array(torchvision.transforms.RandomRotation(degrees = 25)(Image.fromarray(train_dataset[0][0][0,:,:].numpy()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fsGPznZy_9LJ"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for image in torchvision.transforms.FiveCrop(size = 22)(Image.fromarray(train_dataset[0][0][0,:,:].numpy())):\n",
    "  plt.imshow(np.array(image))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j8eykSX7OafI"
   },
   "source": [
    "#### DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KoIU0rUkqzxn"
   },
   "source": [
    "The DataLoader class handles how the dataset is sampled for training or testing. The main functions of it are handling the shuffling of the dataset, the batch size and the parallel loading of data. You will almost always directly use the provided class from PyTorch: (https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vMjeKK5c8531"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle = True, batch_size = 256, num_workers = 8)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = 256, num_workers = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8a-P9v1DONq5"
   },
   "source": [
    "## Creating Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HZOdFHmhcSaa"
   },
   "source": [
    "We will create three models here to exemplify different ways of constructing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PAPwHFSDb5G8"
   },
   "source": [
    "Models are derived classes of the torch.nn.Module class, and are usually composed of several modules. The nn package should contain all the important layers for deep learning: https://pytorch.org/docs/stable/nn.html . Some of these important layers are Linear, MaxPool2d, AvgPool2d, Conv2d, Dropout2d, BatchNorm2d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HapmC19Po8N_"
   },
   "source": [
    "#### Creating with Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xabtMq5QKmfH"
   },
   "source": [
    "The Sequential module (https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential) is one way of creating models, when they are a simple sequence of other modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hv6m4kQaJhQn"
   },
   "outputs": [],
   "source": [
    "model1 = torch.nn.Sequential(torch.nn.Linear(in_features = 28*28, out_features = 100), torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(in_features = 100, out_features = 10))\n",
    "model1 = model1.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QGH1_ACcpEzJ"
   },
   "source": [
    "#### Creating with Module class inheritance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dRo4HI2xeEXe"
   },
   "source": [
    "You can also inherit from the Module class to create a model. You should declare a forward function, which defines what feeding the model with an input does. The backward pass is defined by PyTorch automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yPXx1De_LJvS"
   },
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.convolution_layer_1 = torch.nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = 5)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.maxpooling_layer = torch.nn.MaxPool2d(kernel_size = 2)\n",
    "        self.convolution_layer_2 = torch.nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = 5)\n",
    "        self.fully_connected_layer = torch.nn.Linear(in_features = 256, out_features = 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolution_layer_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpooling_layer(x)\n",
    "        x = self.convolution_layer_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpooling_layer(x)\n",
    "        \n",
    "        #flattening the tensor so that it can serve as input to a linear layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fully_connected_layer(x)\n",
    "        return x\n",
    "      \n",
    "model2 = ConvNet()\n",
    "model2 = model2.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yNGvar75pAu_"
   },
   "source": [
    "#### Weight initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c7XEelUHn534"
   },
   "source": [
    "All weights are initialized using a good initialization rule. But you can also set your own rules using modules from torch.nn.init (https://pytorch.org/docs/master/nn.html#torch-nn-init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QY1ZsV__n5kr"
   },
   "outputs": [],
   "source": [
    "torch.nn.init.xavier_normal_(model2.convolution_layer_1.weight);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VENBt6dypKSA"
   },
   "source": [
    "#### Creating with pre-defined models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mC3n8AemNj_K"
   },
   "source": [
    "PyTorch offers a few pre-built models with optional pretrained weights on ImageNet (https://pytorch.org/docs/stable/torchvision/models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ya677gnZM3vO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /home/u0579755/.torch/models/resnet18-5c106cde.pth\n",
      "52.5%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model3 = torchvision.models.resnet18(pretrained = True)\n",
    "print(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ce0ZYnTU9KWH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=512, out_features=1000, bias=True)\n",
      "Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n"
     ]
    }
   ],
   "source": [
    "print(model3.fc)\n",
    "print(model3.conv1)\n",
    "model3.fc = torch.nn.Linear(in_features = 512, out_features = 10)\n",
    "model3.avgpool = torch.nn.Sequential()\n",
    "model3 = model3.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ip4XamiuOupX"
   },
   "source": [
    "## Other declarations for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vl7HSw_VpRlu"
   },
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SWAtoQqUsKqO"
   },
   "source": [
    "PyTorch has several different kinds of losses predefined (https://pytorch.org/docs/stable/nn.html#loss-functions). For this example, we will use CrossEntropyLoss. It is preferable to use the losses that receive logits instead of probabilities because they are numerically more stable. You should be careful to have a model output which matches what your loss expects as input. Note that the models defined in the Models section above do not have a Softmax output. Instead, we are going to use a loss function that does the softmax for us. Also, the CrossEntropyLoss receives classes as target, and not one-hot vectors. This means we do not need to change the labels from the dataset to feed it to the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RC51DlIUunPK"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p6j_BZlEpTL1"
   },
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_GWQEka1usZd"
   },
   "source": [
    "PyTorch has several different kinds of optimizers predefined (https://pytorch.org/docs/stable/optim.html). You should provide a specific set of weights to each optimizer instance to specify which weights it should optimize. More details on the use of the optimizer will be presented in the training section of this tutorial.\n",
    "\n",
    "We will use the Adam optimizer since usually it is the optimizer that presents the fastest convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6OHm0dUHwMkf"
   },
   "outputs": [],
   "source": [
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr = 0.01)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr = 0.0005)\n",
    "optimizer3 = torch.optim.Adam(model3.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B4nmuwPpOzeH"
   },
   "source": [
    "## Training a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PP6KLwoDw-MD"
   },
   "source": [
    "1) **Loops:** We will have two loops, one for epochs and one for batches\n",
    "\n",
    "2) **Model mode:** Remember to toggle train or eval mode\n",
    "\n",
    "3) **zero_grad:** PyTorch accumulates gradients by default. This means that in the rare cases we want to run backpropagation more than once for the same batch, it will remember the gradients for both backpropagations.  However, for every new batch, we want the gradients to have no influences from the gradients of the previous batch. So at the start of every batch iteration, we call the zero_grad() function. \n",
    "\n",
    "4) **backward:** To run backpropagation, we need to call the backward() function for the loss\n",
    "\n",
    "5) **step:** To update the parameters, we need to call the step function for the optimizer of those parameters\n",
    "\n",
    "6)  **Monitoring:**\n",
    "- We want to monitor a few metrics during training to be able to compare different models, check that the model is learning something and check if the model has converged. For this, in this tutorial we will print the training loss value. It is also important to follow the training and validation accuracy for every epoch. You could do this more frequently than every epoch, if needed.\n",
    "- TensorBoard, for TensorFlow, is an even better way for monitoring training. For PyTorch, you can use tensorboardX (https://github.com/lanpa/tensorboardX) to create TensorBoard files, and then use TensorBoard to visualize them.\n",
    "\n",
    "7)  **Memory leak:** Be careful not to store tensors from several different iterations, as this may cause memory leaks. Use .item() to convert a single-valued tensor to a python number, or .cpu().numpy() to convert a larger tensor to a numpy array before storing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p_PB0WBg3t_H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "loss1: 0.5805379294968666\n",
      "loss2: 0.8400896666531867\n",
      "loss3: 0.18907283061045282\n",
      "Epoch 1\n",
      "loss1: 0.2595943514336931\n",
      "loss2: 0.19863181913152655\n",
      "loss3: 0.03004087877875947\n",
      "Epoch 2\n",
      "loss1: 0.22444843003090392\n",
      "loss2: 0.1293274666876235\n",
      "loss3: 0.01729122882590015\n",
      "Epoch 3\n",
      "loss1: 0.20995230922039518\n",
      "loss2: 0.10241275732504561\n",
      "loss3: 0.013662166734959217\n",
      "Epoch 4\n",
      "loss1: 0.19130834927584262\n",
      "loss2: 0.08645372189422872\n",
      "loss3: 0.011071503385554682\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5): # 1)\n",
    "  \n",
    "    model1.train() # 2)\n",
    "    model2.train() # 2)\n",
    "    model3.train() # 2)\n",
    "    \n",
    "    losses1 = []\n",
    "    losses2 = []\n",
    "    losses3 = []\n",
    "    for images, targets in train_loader: # 1)\n",
    "      \n",
    "        optimizer1.zero_grad() # 3)\n",
    "        optimizer2.zero_grad() # 3)\n",
    "        optimizer3.zero_grad() # 3)\n",
    "        \n",
    "        #putting variables on GPU since model is on GPU\n",
    "        images = images.cuda()\n",
    "        targets = targets.cuda()\n",
    "        \n",
    "        #running each model by adapting the imagees tensor to the expected input size of each model\n",
    "        out1 = model1(images.view(images.size(0), -1))\n",
    "        out2 = model2(images)\n",
    "        out3 = model3(images.expand(-1, 3, -1, -1))\n",
    "        \n",
    "        #calculating the losses with the defined criterion\n",
    "        loss1 = criterion(out1, targets)\n",
    "        loss2 = criterion(out2, targets)\n",
    "        loss3 = criterion(out3, targets)\n",
    "        \n",
    "        loss1.backward() # 4)\n",
    "        loss2.backward() # 4)\n",
    "        loss3.backward() # 4)\n",
    "        \n",
    "        optimizer1.step() # 5)\n",
    "        optimizer2.step() # 5)\n",
    "        optimizer3.step() # 5)\n",
    "        \n",
    "        losses1.append(loss1.item()) # 6) 7)\n",
    "        losses2.append(loss2.item()) # 6) 7)\n",
    "        losses3.append(loss3.item()) # 6) 7)\n",
    "        \n",
    "    print('Epoch ' + str(epoch)) # 6)\n",
    "    print('loss1: ' + str(np.mean(losses1))) # 6)\n",
    "    print('loss2: ' + str(np.mean(losses2))) # 6)\n",
    "    print('loss3: ' + str(np.mean(losses3))) # 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8eeN3NzWPmoa"
   },
   "source": [
    "## Evaluating model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-L079YjEzNG7"
   },
   "source": [
    "Evaluating the model has a similar structure to training the model, but there is no need for gradients and optimizers, and you should toggle the eval mode of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KuVJ8icd9ZTX",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy1: 0.9344\n",
      "accuracy2: 0.9782\n",
      "accuracy3: 0.9902\n"
     ]
    }
   ],
   "source": [
    "#turning off gradients will make model run slightly faster\n",
    "with torch.no_grad():\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    model3.eval()\n",
    "\n",
    "    n_corrects1 = 0\n",
    "    n_corrects2 = 0\n",
    "    n_corrects3 = 0\n",
    "    for images, targets in val_loader:\n",
    "\n",
    "        images = images.cuda()\n",
    "        targets = targets.cuda()\n",
    "\n",
    "        out1 = model1(images.view(images.size(0), -1))\n",
    "        out2 = model2(images)\n",
    "        out3 = model3(images.expand(-1, 3, -1, -1))\n",
    "        \n",
    "        # Counting how many correct predictions were made for this batch\n",
    "        # to get the prediction of each model, find the index \n",
    "        # of the maximum logit in each batch example\n",
    "        n_corrects1 += torch.sum(torch.argmax(out1, dim = 1)==targets).item()\n",
    "        n_corrects2 += torch.sum(torch.argmax(out2, dim = 1)==targets).item()\n",
    "        n_corrects3 += torch.sum(torch.argmax(out3, dim = 1)==targets).item()\n",
    "\n",
    "    print('accuracy1: ' + str(n_corrects1/len(val_dataset)))\n",
    "    print('accuracy2: ' + str(n_corrects2/len(val_dataset)))\n",
    "    print('accuracy3: ' + str(n_corrects3/len(val_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BMbJTKvc9KWw"
   },
   "source": [
    "Now we combine training and evaluation in the same cell to follow validation and training during training. The code is just a combined copy and paste from the two previous code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IN9SwFve9KWy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "loss1: 0.17906305552163024\n",
      "loss2: 0.07529026395145882\n",
      "loss3: 0.007022672495309343\n",
      "Validation:\n",
      "accuracy1: 0.9483\n",
      "accuracy2: 0.982\n",
      "accuracy3: 0.9901\n",
      "Training:\n",
      "accuracy1: 0.9504833333333333\n",
      "accuracy2: 0.9791833333333333\n",
      "accuracy3: 0.9979166666666667\n",
      "Epoch 1\n",
      "loss1: 0.1748753548936641\n",
      "loss2: 0.06841908839947365\n",
      "loss3: 0.006563607813354503\n",
      "Validation:\n",
      "accuracy1: 0.9472\n",
      "accuracy2: 0.9818\n",
      "accuracy3: 0.9909\n",
      "Training:\n",
      "accuracy1: 0.9503666666666667\n",
      "accuracy2: 0.9805\n",
      "accuracy3: 0.9986666666666667\n",
      "Epoch 2\n",
      "loss1: 0.17033885244993455\n",
      "loss2: 0.06284375091657994\n",
      "loss3: 0.009207224627421733\n",
      "Validation:\n",
      "accuracy1: 0.9499\n",
      "accuracy2: 0.983\n",
      "accuracy3: 0.9922\n",
      "Training:\n",
      "accuracy1: 0.9520166666666666\n",
      "accuracy2: 0.9830333333333333\n",
      "accuracy3: 0.9983833333333333\n",
      "Epoch 3\n",
      "loss1: 0.16482719303445614\n",
      "loss2: 0.05774467729349086\n",
      "loss3: 0.00617296487152418\n",
      "Validation:\n",
      "accuracy1: 0.9508\n",
      "accuracy2: 0.9836\n",
      "accuracy3: 0.993\n",
      "Training:\n",
      "accuracy1: 0.9525\n",
      "accuracy2: 0.9831333333333333\n",
      "accuracy3: 0.9992\n",
      "Epoch 4\n",
      "loss1: 0.16056675387823835\n",
      "loss2: 0.054252696314707714\n",
      "loss3: 0.007576190706025413\n",
      "Validation:\n",
      "accuracy1: 0.9452\n",
      "accuracy2: 0.9845\n",
      "accuracy3: 0.9917\n",
      "Training:\n",
      "accuracy1: 0.9475833333333333\n",
      "accuracy2: 0.9843833333333334\n",
      "accuracy3: 0.9991166666666667\n",
      "Epoch 5\n",
      "loss1: 0.16121430958205082\n",
      "loss2: 0.05073481029335489\n",
      "loss3: 0.005906588023409564\n",
      "Validation:\n",
      "accuracy1: 0.9493\n",
      "accuracy2: 0.9841\n",
      "accuracy3: 0.9932\n",
      "Training:\n",
      "accuracy1: 0.9558333333333333\n",
      "accuracy2: 0.9847\n",
      "accuracy3: 0.9997333333333334\n",
      "Epoch 6\n",
      "loss1: 0.16006005167009982\n",
      "loss2: 0.04760592730438456\n",
      "loss3: 0.005897650007713348\n",
      "Validation:\n",
      "accuracy1: 0.9474\n",
      "accuracy2: 0.9847\n",
      "accuracy3: 0.9909\n",
      "Training:\n",
      "accuracy1: 0.9538833333333333\n",
      "accuracy2: 0.9847666666666667\n",
      "accuracy3: 0.9987666666666667\n",
      "Epoch 7\n",
      "loss1: 0.15503127349818008\n",
      "loss2: 0.045363167609940185\n",
      "loss3: 0.004482014925080411\n",
      "Validation:\n",
      "accuracy1: 0.9476\n",
      "accuracy2: 0.9867\n",
      "accuracy3: 0.9932\n",
      "Training:\n",
      "accuracy1: 0.9532\n",
      "accuracy2: 0.9874333333333334\n",
      "accuracy3: 0.9996666666666667\n",
      "Epoch 8\n",
      "loss1: 0.1542807825702302\n",
      "loss2: 0.04331098556756339\n",
      "loss3: 0.005225105177947974\n",
      "Validation:\n",
      "accuracy1: 0.9503\n",
      "accuracy2: 0.985\n",
      "accuracy3: 0.9917\n",
      "Training:\n",
      "accuracy1: 0.9542\n",
      "accuracy2: 0.98685\n",
      "accuracy3: 0.99925\n",
      "Epoch 9\n",
      "loss1: 0.15351791262943693\n",
      "loss2: 0.0410713937153366\n",
      "loss3: 0.004234587153459483\n",
      "Validation:\n",
      "accuracy1: 0.9509\n",
      "accuracy2: 0.9872\n",
      "accuracy3: 0.9911\n",
      "Training:\n",
      "accuracy1: 0.9578333333333333\n",
      "accuracy2: 0.9883\n",
      "accuracy3: 0.99765\n"
     ]
    }
   ],
   "source": [
    "#continuing training but now monitoring accuracy for training and validation\n",
    "for epoch in range(10): \n",
    "    model2.train() \n",
    "    model3.train() \n",
    "    \n",
    "    losses1 = []\n",
    "    losses2 = []\n",
    "    losses3 = []\n",
    "    \n",
    "    #training\n",
    "    for images, targets in train_loader: \n",
    "      \n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad() \n",
    "        optimizer3.zero_grad() \n",
    "        \n",
    "        images = images.cuda()\n",
    "        targets = targets.cuda()\n",
    "\n",
    "        out1 = model1(images.view(images.size(0), -1))\n",
    "        out2 = model2(images)\n",
    "        out3 = model3(images.expand(-1, 3, -1, -1))\n",
    "        \n",
    "\n",
    "        loss1 = criterion(out1, targets)\n",
    "        loss2 = criterion(out2, targets)\n",
    "        loss3 = criterion(out3, targets)\n",
    "        \n",
    "        loss1.backward() \n",
    "        loss2.backward() \n",
    "        loss3.backward() \n",
    "        \n",
    "        optimizer1.step() \n",
    "        optimizer2.step() \n",
    "        optimizer3.step() \n",
    "        \n",
    "        losses1.append(loss1.item()) \n",
    "        losses2.append(loss2.item()) \n",
    "        losses3.append(loss3.item()) \n",
    "        \n",
    "    print('Epoch ' + str(epoch)) \n",
    "    print('loss1: ' + str(np.mean(losses1))) \n",
    "    print('loss2: ' + str(np.mean(losses2))) \n",
    "    print('loss3: ' + str(np.mean(losses3))) \n",
    "    \n",
    "    #accuracy for validation\n",
    "    with torch.no_grad():\n",
    "        model1.eval()\n",
    "        model2.eval()\n",
    "        model3.eval()\n",
    "\n",
    "        n_corrects1 = 0\n",
    "        n_corrects2 = 0\n",
    "        n_corrects3 = 0\n",
    "        for images, targets in val_loader:\n",
    "\n",
    "            images = images.cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "            out1 = model1(images.view(images.size(0), -1))\n",
    "            out2 = model2(images)\n",
    "            out3 = model3(images.expand(-1, 3, -1, -1))\n",
    "\n",
    "            n_corrects1 += torch.sum(torch.argmax(out1, dim = 1)==targets).item()\n",
    "            n_corrects2 += torch.sum(torch.argmax(out2, dim = 1)==targets).item()\n",
    "            n_corrects3 += torch.sum(torch.argmax(out3, dim = 1)==targets).item()\n",
    "        \n",
    "        print('Validation:')\n",
    "        print('accuracy1: ' + str(n_corrects1/len(val_dataset)))\n",
    "        print('accuracy2: ' + str(n_corrects2/len(val_dataset)))\n",
    "        print('accuracy3: ' + str(n_corrects3/len(val_dataset)))\n",
    "        \n",
    "    #accuracy for training\n",
    "    with torch.no_grad():\n",
    "        model1.eval()\n",
    "        model2.eval()\n",
    "        model3.eval()\n",
    "\n",
    "        n_corrects1 = 0\n",
    "        n_corrects2 = 0\n",
    "        n_corrects3 = 0\n",
    "        for images, targets in train_loader:\n",
    "\n",
    "            images = images.cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "            out1 = model1(images.view(images.size(0), -1))\n",
    "            out2 = model2(images)\n",
    "            out3 = model3(images.expand(-1, 3, -1, -1))\n",
    "\n",
    "            n_corrects1 += torch.sum(torch.argmax(out1, dim = 1)==targets).item()\n",
    "            n_corrects2 += torch.sum(torch.argmax(out2, dim = 1)==targets).item()\n",
    "            n_corrects3 += torch.sum(torch.argmax(out3, dim = 1)==targets).item()\n",
    "        print('Training:')\n",
    "        print('accuracy1: ' + str(n_corrects1/len(train_dataset)))\n",
    "        print('accuracy2: ' + str(n_corrects2/len(train_dataset)))\n",
    "        print('accuracy3: ' + str(n_corrects3/len(train_dataset)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PyTorch_tutorial.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "env_dir",
   "language": "python",
   "name": "env_dir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
