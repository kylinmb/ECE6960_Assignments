{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Segmentation\n",
    "\n",
    "You are expected to complete this notebook with lines of code, plots and texts. You will need to create new cells with original code or text for your analyses. This assignment has a total of 100 points and 5 questions.\n",
    "\n",
    "Using Canvas, you will deliver the notebook file (.ipynb) with cells executed and outputs visible.\n",
    "- You should use PyTorch 1.0 or later as your deep learning framework. If you need to import a different package than the ones already imported, ask the TA if you can do it.\n",
    "- No other data than the dataset variables provided should be used, and training, validation and testing splits should be the same as the ones provided.\n",
    "- The cell outputs present in your delivered notebook should be reproducible by us by running your notebook cells in order.\n",
    "- All code must be your own work. Code cannot be copied from another source or student. You may copy code from cells that were pre-defined in this notebook if you think it is useful for use in another part of the notebook.\n",
    "- All images must be generated from data generated in your code. Do NOT import/display images generated outside your code.\n",
    "- Your analysis must be your own, but if you quote text or equations from another source make sure to cite the reference.\n",
    "- It is assumed that PyTorch is already installed according to the CADE or the COLAB tutorial.\n",
    "- For all analyses you write, in the case that you ran code to get to its conclusions, you should provide code that provides evidence for these conclusions. This code should print the numbers that are cited, or plot a graph, or print a table, or, even better, a combination of these. Do not erase or comment code and its outputs in the case you cite them in your analyses.\n",
    "- You should provide evidence that your code satisfies all the constraints of the questions. For showing that numeric constraints are satisfied, provide some kind of printed number, and not only a graph.\n",
    "\n",
    "Other notes:\n",
    "- Cells should be run in order, using Shift+Enter.\n",
    "- Read all the provided code cells and its comments, as it contains variables and information that you may need to use to complete the notebook.\n",
    "- To create a text cell, create it with the \"+\" button and change its type from \"Code\" to \"Markdown\" using the top menu. To modify a text cell, double click on it.\n",
    "- If you are interested, you can check detail on formatting markdown text here: https://medium.com/ibm-data-science-experience/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed\n",
    "- The accuracies provided as required for each question are there to make sure you work enough on each model to get a good result. Part of the grade is based on this.\n",
    "- We assume a GPU of at least 4GB of memory is available. If you want to try running the assignment with a GPU that has less than that, you can try changing the argument passed when calling the ```define_gpu_to_use``` function.  If you are getting out-of-memory errors for the GPU, you may want to check what is occupying the GPU memory by using the command ```!nvidia-smi```, which gives a report of the use of the GPU. \n",
    "- A few PyTorch details to remember:\n",
    "    - Remember to toggle train/eval mode for your model\n",
    "    - Remember to reset the gradients with ```zero_grad()``` before each call to ```backward()```\n",
    "    - Remember to check if the loss you are using receives logits or probabilities, and adapt your model output accordingly.\n",
    "    - Remember to reinstantiate your model every time you are starting a new training, so that weights are reset.\n",
    "    - Remember to pass to the optimizer the set of parameters for the model you want to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imageio in /home/u0579755/env_dir/lib/python3.6/site-packages (2.5.0)\n",
      "Requirement already satisfied: pillow in /home/u0579755/env_dir/lib/python3.6/site-packages (from imageio) (5.4.1)\n",
      "Requirement already satisfied: numpy in /home/u0579755/env_dir/lib/python3.6/site-packages (from imageio) (1.16.0)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /home/u0579755/env_dir/lib/python3.6/site-packages (3.0.2)\n",
      "Requirement already satisfied: numpy>=1.10.0 in /home/u0579755/env_dir/lib/python3.6/site-packages (from matplotlib) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/u0579755/env_dir/lib/python3.6/site-packages (from matplotlib) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/u0579755/env_dir/lib/python3.6/site-packages (from matplotlib) (2.7.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/u0579755/env_dir/lib/python3.6/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/u0579755/env_dir/lib/python3.6/site-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/u0579755/env_dir/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /home/u0579755/env_dir/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (40.8.0)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: scikit-image in /home/u0579755/env_dir/lib/python3.6/site-packages (0.15.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /home/u0579755/env_dir/lib/python3.6/site-packages (from scikit-image) (1.2.1)\n",
      "Requirement already satisfied: imageio>=2.0.1 in /home/u0579755/env_dir/lib/python3.6/site-packages (from scikit-image) (2.5.0)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /home/u0579755/env_dir/lib/python3.6/site-packages (from scikit-image) (1.0.3)\n",
      "Requirement already satisfied: pillow>=4.3.0 in /home/u0579755/env_dir/lib/python3.6/site-packages (from scikit-image) (5.4.1)\n",
      "Requirement already satisfied: networkx>=2.0 in /home/u0579755/env_dir/lib/python3.6/site-packages (from scikit-image) (2.3)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /home/u0579755/env_dir/lib/python3.6/site-packages (from scikit-image) (3.0.2)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /home/u0579755/env_dir/lib/python3.6/site-packages (from scipy>=0.17.0->scikit-image) (1.16.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/u0579755/env_dir/lib/python3.6/site-packages (from networkx>=2.0->scikit-image) (4.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/u0579755/env_dir/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/u0579755/env_dir/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.7.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/u0579755/env_dir/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/u0579755/env_dir/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.3.1)\n",
      "Requirement already satisfied: six in /home/u0579755/env_dir/lib/python3.6/site-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /home/u0579755/env_dir/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image) (40.8.0)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: scikit-learn in /home/u0579755/env_dir/lib/python3.6/site-packages (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /home/u0579755/env_dir/lib/python3.6/site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /home/u0579755/env_dir/lib/python3.6/site-packages (from scikit-learn) (1.2.1)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install imageio\n",
    "!pip3 install matplotlib\n",
    "!pip3 install scikit-image\n",
    "!pip3 install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries used in the rest of the code\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import tarfile\n",
    "import imageio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import zipfile\n",
    "import collections\n",
    "from skimage import morphology\n",
    "from skimage.measure import block_reduce\n",
    "import scipy\n",
    "from torch.utils.data import Dataset\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from sklearn.metrics import f1_score\n",
    "import copy\n",
    "import random\n",
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking what kind of system you are using\n",
    "try:\n",
    "  import google.colab\n",
    "  from google.colab import drive\n",
    "  from google.colab import files\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "try:\n",
    "    hostname = !hostname\n",
    "    if 'lab' in hostname[0] and '.eng.utah.edu' in hostname[0]:\n",
    "        IN_CADE = True\n",
    "    else:\n",
    "        IN_CADE = False\n",
    "except:\n",
    "    IN_CADE = False\n",
    "\n",
    "assert(not IN_CADE or not IN_COLAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with this function you set the value of the environment variable CUDA_VISIBLE_DEVICES\n",
    "# to set which GPU to use\n",
    "# it also reserves this amount of memory for your exclusive use. This might be important for \n",
    "# not having other people using the resources you need in shared systems\n",
    "# the homework was tested in a GPU with 4GB of memory, and running this function will require at least\n",
    "# as much\n",
    "# if you want to test in a GPU with less memory, you can call this function\n",
    "# with the argument minimum_memory_mb specifying how much memory from the GPU you want to reserve\n",
    "def define_gpu_to_use(minimum_memory_mb = 3800):\n",
    "    gpu_to_use = None\n",
    "    try: \n",
    "        os.environ['CUDA_VISIBLE_DEVICES']\n",
    "        print('GPU already assigned before: ' + str(os.environ['CUDA_VISIBLE_DEVICES']))\n",
    "        return\n",
    "    except:\n",
    "        pass\n",
    "    torch.cuda.empty_cache()\n",
    "    for i in range(16):\n",
    "        free_memory = !nvidia-smi --query-gpu=memory.free -i $i --format=csv,nounits,noheader\n",
    "        if free_memory[0] == 'No devices were found':\n",
    "            break\n",
    "        free_memory = int(free_memory[0])\n",
    "        if free_memory>minimum_memory_mb-500:\n",
    "            gpu_to_use = i\n",
    "            break\n",
    "    if gpu_to_use is None:\n",
    "        print('Could not find any GPU available with the required free memory of ' +str(minimum_memory_mb) + 'MB. Please use a different system for this assignment.')\n",
    "    else:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_to_use)\n",
    "        print('Chosen GPU: ' + str(gpu_to_use))\n",
    "        x = torch.rand((256,1024,minimum_memory_mb-500)).cuda()\n",
    "        x = torch.rand((1,1)).cuda()\n",
    "        del x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find any GPU available with the required free memory of 3800MB. Please use a different system for this assignment.\n"
     ]
    }
   ],
   "source": [
    "if IN_CADE:\n",
    "    #setting the gpu that will be used, testing if it has enough available memory, and reserving the needed memory\n",
    "    define_gpu_to_use()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining functions used to load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use datasets of retinal digital images and segment the blood vessels from it. We are going to use the DRIVE dataset (https://www.isi.uu.nl/Research/Databases/DRIVE/) and the STARE dataset (http://cecas.clemson.edu/~ahoover/stare/) together in this assignment. We are going to use our own train/validation/test splits, even though the DRIVE dataset provides its own split. Please download the files named \"DRIVE.zip\" (register in https://www.isi.uu.nl/Research/Databases/DRIVE/download.php and follow the instructions), \"stare-images.tar\" (http://cecas.clemson.edu/~ahoover/stare/probing/stare-images.tar), \"labels-vk.tar\" (http://cecas.clemson.edu/~ahoover/stare/probing/labels-vk.tar) and put in the same folder as this notebook file so that the dataset can be loaded. \n",
    "\n",
    "The dataset also contains masks since the images are not shaped as squares originally, but are only padded so that we can fit them to a traditional CNN. The masks contain the information of where the original image is and where the padding is located. These masks should be used to limit where outputs are backpropagated for training and what region of the image should be used for scoring. In the mask, 1 means that it is part of the original image, and 0 means that it in the zero-padded region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete small regions (<size) of binary images\n",
    "def remove_small_regions(img, size):\n",
    "    img = morphology.remove_small_objects(img, size)\n",
    "    img = morphology.remove_small_holes(img, size)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize the images of the dataset to be half the height and half the width of the original images, so \n",
    "# that models states can fit on the GPU memory\n",
    "def resize_img(img):\n",
    "    if len(img.shape)==3:\n",
    "        img = np.array(Image.fromarray(img).resize(((img.shape[1]+1)//2,(img.shape[0]+1)//2), PIL.Image.BILINEAR))\n",
    "    else:\n",
    "        img = block_reduce(img, block_size=(2, 2), func=np.max)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzips, loads and calculates masks for images from the stare dataset\n",
    "def stare_read_images(tar_filename, dest_folder, do_mask = False):\n",
    "    tar = tarfile.open(tar_filename)\n",
    "    tar.extractall(dest_folder)\n",
    "    tar.close()\n",
    "    all_images = []\n",
    "    all_masks = []\n",
    "    for item in sorted(os.listdir(dest_folder)):\n",
    "        if item.endswith('gz'):\n",
    "            with gzip.open(dest_folder + item, 'rb') as f_in:\n",
    "                with open(dest_folder + item[:-3], 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "            os.remove(dest_folder + item) \n",
    "            img = imageio.imread(dest_folder + item[:-3])\n",
    "            if len(img.shape) == 3:\n",
    "                img = np.pad(img , ((1,2), (2,2),(0,0)), mode = 'constant')\n",
    "            else:\n",
    "                img = np.pad(img , ((1,2), (2,2)), mode = 'constant')\n",
    "            img = resize_img(img)\n",
    "            img = img/255.\n",
    "            img = img.astype(np.float32)\n",
    "            if len(img.shape) == 2:\n",
    "                img = img.astype(np.float32)\n",
    "                img = np.expand_dims(img, axis = 2)\n",
    "            all_images.append(img)\n",
    "            if do_mask:\n",
    "                mask = (1-remove_small_regions(np.prod((img<50/255.)*1.0, axis = 2)>0.5, 1000))*1.0\n",
    "                mask = np.expand_dims(mask, axis = 2)\n",
    "                all_masks.append(mask.astype(np.float32))\n",
    "    if do_mask:\n",
    "        return all_images, all_masks\n",
    "    else:\n",
    "        return all_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzips and loads masks for images from the stare dataset\n",
    "def drive_read_images(filetype, dest_folder):\n",
    "    zip_ref = zipfile.ZipFile('DRIVE.zip', 'r')\n",
    "    zip_ref.extractall('datasets/drive')\n",
    "    zip_ref.close()\n",
    "    all_images = []\n",
    "    for item in sorted(os.listdir(dest_folder)):\n",
    "        if item.endswith(filetype):\n",
    "            img = imageio.imread(dest_folder + item)\n",
    "            if len(img.shape) == 3:\n",
    "                img = np.pad(img , ((12,12), (69,70),(0,0)), mode = 'constant')\n",
    "            else:\n",
    "                img = np.pad(img , ((12,12), (69,70)), mode = 'constant')\n",
    "            img = resize_img(img)\n",
    "            img = img/255.\n",
    "            img = img.astype(np.float32)\n",
    "            if len(img.shape) == 2:\n",
    "                img = img.astype(np.float32)\n",
    "                img = np.expand_dims(img, axis = 2)\n",
    "            all_images.append(img)\n",
    "    return all_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all images and put them on a list of list of arrays.\n",
    "# on the inner lists, first element is an input image, second element is a segmentation groundtruth\n",
    "# and third element is a mask to show where the input image is valid, in contrast to where it was padded\n",
    "def get_retina_array():\n",
    "    stare_images, stare_mask = stare_read_images(\"stare-images.tar\", 'datasets/stare/images/', do_mask = True)            \n",
    "    stare_segmentation = stare_read_images(\"labels-vk.tar\", 'datasets/stare/segmentations/')   \n",
    "    drive_training_images = drive_read_images('tif', 'datasets/drive/DRIVE/training/images/')\n",
    "    drive_test_images = drive_read_images('tif', 'datasets/drive/DRIVE/test/images/')\n",
    "    drive_training_segmentation = drive_read_images('gif', 'datasets/drive/DRIVE/training/1st_manual/')\n",
    "    drive_test_segmentation = drive_read_images('gif', 'datasets/drive/DRIVE/test/1st_manual/')\n",
    "    drive_training_mask = drive_read_images('gif', 'datasets/drive/DRIVE/training/mask/')\n",
    "    drive_test_mask = drive_read_images('gif', 'datasets/drive/DRIVE/test/mask/')\n",
    "    return list(zdip(stare_images+drive_training_images+drive_test_images, \n",
    "                           stare_segmentation+drive_training_segmentation+drive_test_segmentation, \n",
    "                           stare_mask + drive_training_mask + drive_test_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split can be 'train', 'val', and 'test'\n",
    "#this is the function that splits a dataset into training, validation and testing set\n",
    "#We are using a split of 70%-10%-20%, for train-val-test, respectively\n",
    "#this function is used internally to the defined dataset classes\n",
    "def get_split(array_to_split, split):\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(array_to_split)\n",
    "    np.random.seed()\n",
    "    if split == 'train':\n",
    "        array_to_split = array_to_split[:int(len(array_to_split)*0.7)]\n",
    "    elif split == 'val':\n",
    "        array_to_split = array_to_split[int(len(array_to_split)*0.7):int(len(array_to_split)*0.8)]\n",
    "    elif split == 'test':\n",
    "        array_to_split = array_to_split[int(len(array_to_split)*0.8):]\n",
    "    return array_to_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for segmentations tasks, the exact transformations that are applied to \n",
    "# the input image should be applied, down to the random number used, should\n",
    "# also be applied to the ground truth and to the masks. We redefine a few of\n",
    "# PyTorch classes \n",
    "\n",
    "#apply transoforms to all tensors in list x \n",
    "def _iterate_transforms(transform, x):\n",
    "    for i, xi in enumerate(x):\n",
    "        x[i] = transform(x[i])\n",
    "    return x\n",
    "\n",
    "#redefining composed transform so that it uses the _iterate_transforms function\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for transform in self.transforms:\n",
    "            x = _iterate_transforms(transform, x) \n",
    "        return x\n",
    "\n",
    "#class to rerandomize the vertical flip transformation   \n",
    "class RandomVerticalFlipGenerator(object):\n",
    "    def __call__(self, img):\n",
    "        self.random_n = random.uniform(0, 1)\n",
    "        return img\n",
    "\n",
    "#class to perform vertical flip using randomization provided by gen\n",
    "class RandomVerticalFlip(object):\n",
    "    def __init__(self, gen):\n",
    "        self.gen = gen\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if self.gen.random_n < 0.5:\n",
    "            return torch.flip(img, [2])\n",
    "        return img\n",
    "\n",
    "#class to rerandomize the horizontal flip transformation   \n",
    "class RandomHorizontalFlipGenerator(object):\n",
    "    def __call__(self, img):\n",
    "        self.random_n = random.uniform(0, 1)\n",
    "        return img\n",
    "\n",
    "#class to perform horizontal flip using randomization provided by gen\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, gen):\n",
    "        self.gen = gen\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if self.gen.random_n < 0.5:\n",
    "            return torch.flip(img, [1])\n",
    "        return img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset class for the retina dataset\n",
    "# each item of the dataset is a tuple with three items:\n",
    "# - the first element is the input image to be segmented \n",
    "# - the second element is the segmentation ground truth image \n",
    "# - the third element is a mask to know what parts of the input image should be used (for training and for scoring)\n",
    "class RetinaDataset(Dataset):\n",
    "    def transpose_first_index(self, x):\n",
    "        x2 = (np.transpose(x[0], [2,0,1]), np.transpose(x[1], [2,0,1]), np.transpose(x[2], [2,0,1]))\n",
    "        return x2\n",
    "    \n",
    "    def __init__(self, retina_array, split = 'train', do_transform=False):\n",
    "        indexes_this_split = get_split(np.arange(len(retina_array), dtype = np.int), split)\n",
    "        self.retina_array = [self.transpose_first_index(retina_array[i]) for i in indexes_this_split]\n",
    "        self.split = split\n",
    "        self.do_transform = do_transform\n",
    "    def __getitem__(self, index):\n",
    "        sample = [torch.FloatTensor(x) for x in self.retina_array[index]]\n",
    "        if self.do_transform:\n",
    "            v_gen = RandomVerticalFlipGenerator()\n",
    "            h_gen = RandomHorizontalFlipGenerator()\n",
    "            t = Compose([\n",
    "                v_gen,\n",
    "                RandomVerticalFlip(gen=v_gen),\n",
    "                h_gen,\n",
    "                RandomHorizontalFlip(gen=h_gen),\n",
    "            ])\n",
    "            sample = t(sample)\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.retina_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and visualizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u0579755/env_dir/lib/python3.6/site-packages/imageio/plugins/_tifffile.py:8388: UserWarning: unexpected end of LZW stream (code 0)\n",
      "  warnings.warn('unexpected end of LZW stream (code %i)' % code)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'zdip' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-bd3fef7498fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mretina_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_retina_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#datase to use for training:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetinaDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretina_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#dataset to use for validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetinaDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretina_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-80c9262ee7bb>\u001b[0m in \u001b[0;36mget_retina_array\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdrive_training_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrive_read_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gif'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'datasets/drive/DRIVE/training/mask/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdrive_test_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrive_read_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gif'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'datasets/drive/DRIVE/test/mask/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     return list(zdip(stare_images+drive_training_images+drive_test_images, \n\u001b[0m\u001b[1;32m     14\u001b[0m                            \u001b[0mstare_segmentation\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdrive_training_segmentation\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdrive_test_segmentation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                            stare_mask + drive_training_mask + drive_test_mask))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'zdip' is not defined"
     ]
    }
   ],
   "source": [
    "retina_array = get_retina_array()\n",
    "#datase to use for training:\n",
    "train_dataset = RetinaDataset(retina_array, do_transform = True)\n",
    "#dataset to use for validation\n",
    "val_dataset = RetinaDataset(retina_array, split = 'val')\n",
    "#dataset to use for testing\n",
    "test_dataset = RetinaDataset(retina_array, split = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualing a few cases in the training set\n",
    "for batch_idx, (data, segmentation, mask) in enumerate( RetinaDataset(retina_array)):\n",
    "    if batch_idx%15 == 0: \n",
    "        plt.figure()\n",
    "        plt.title(\"Input Image\")\n",
    "        plt.imshow(data[:,:,:].permute([1,2,0]).cpu().numpy())\n",
    "        plt.figure()\n",
    "        plt.title(\"Segmentation ground truth\")\n",
    "        plt.imshow(segmentation[0,:,:].cpu().numpy())\n",
    "        plt.figure()\n",
    "        plt.title(\"Mask\")\n",
    "        plt.imshow(mask[0,:,:].cpu().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a scoring function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure how good our segmentation results are, we are going to use the F1 score. This score is the harmonic mean between precision and recall, considering the foreground as the positive class and the background as the negative class. The score goes from 0 to 1, with 1 the best score possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this function to score your models\n",
    "def get_score_model(model, data_loader):\n",
    "    #toggle model to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    #turn off gradients since they will not be used here\n",
    "    # this is to make the inference faster\n",
    "    with torch.no_grad():\n",
    "        logits_predicted = np.zeros([0, 1, 304, 352])\n",
    "        segmentations = np.zeros([0, 1, 304, 352])\n",
    "        #run through several batches, does inference for each and store inference results\n",
    "        # and store both target labels and inferenced scores \n",
    "        for image, segmentation, mask  in data_loader:\n",
    "            image = image.cuda()\n",
    "            logit_predicted = model(image)\n",
    "            logits_predicted = np.concatenate((logits_predicted, logit_predicted.cpu().detach().numpy()*mask.numpy()), axis = 0)\n",
    "            segmentations = np.concatenate((segmentations, segmentation.cpu().detach().numpy()*mask.numpy()), axis = 0)   \n",
    "    #returns a list of scores, one for each of the labels\n",
    "    return f1_score(segmentations.reshape([-1]), logits_predicted.reshape([-1])>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1 (13 points)\n",
    "Justify why data augmentation usually helps in improving scores in deep learning tasks, and why horizontal flipping and vertical flipping make sense for this dataset. Would they both make sense in a natural image dataset? List at least one other kind of data augmentation that could also be applied for the retina blood vessels segmentation, and justify why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why data augmentation improves scores\n",
    "Data augmentation helps improve scores because it \"can teach the network the desiere invariance and robustenss properties, when only few training samples are avialable\" [1]  \n",
    "\n",
    "Horizontally and vertically flipping make sense for this dataset because those invariants already exist. The optic nerve is a good landmark to recognize that the dataset includes these different invariants already; it can be seen on the right or left in different images as well as closer to the top or bottom in others.  \n",
    "\n",
    "Vertical flipping would not make sense for natural images. Think of an image of a cat sitting on a ground. If vertically flipped, the cat would now be upside down and the sky and ground would be flipped. The upside down cat is unlikely (never going) to occur in the test dataset or in future predictions. The upside down cat is not an invariant we would want our CNN to learn.\n",
    "\n",
    "Rotatoing the images left or right slightly. This is a similar invariant to the flipping where there is no expected location for the optic nerve or any of the landmarks to appear. Another invariant might be zooming in, looking through the datset it seemed there were some images that appeared zoomed in where the veins appeared larege. Thsi is another invariant already in the dataset and one we might want our CNN to learn.\n",
    "\n",
    "[1] Ronneberger, O., Fischer, P., & Brox, T. (2015). U-net: Convolutional networks for biomedical image segmentation. In Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) (Vol. 9351, pp. 234â€“241). Springer Verlag. https://doi.org/10.1007/978-3-319-24574-4_28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2 (11 points)\n",
    "Check how balanced the dataset is, showing how many negative labels there are for each positive label. Use this information to change the weighting of the positive class in the loss for Question 3 and Question 5, and explain why in some cases using the weighting helps in improving performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the given data sets calculates how many values are positive vs negative\n",
    "# for the mask\n",
    "def checkDatasetBalance(dataset):\n",
    "    total_positive = 0\n",
    "    total_negative = 0\n",
    "    for batch_idx, (data, segmentation, mask) in enumerate(dataset):\n",
    "        segmentation_np = segmentation.cpu().numpy()\n",
    "        mask_np = mask.cpu().numpy()\n",
    "        positive = np.sum(segmentation_np == 1)\n",
    "        background = np.sum(mask_np == 0)\n",
    "        negative = np.sum(segmentation_np == 0) - background\n",
    "        total_positive += positive\n",
    "        total_negative += negative \n",
    "    return total_positive, total_negative\n",
    "        \n",
    "\n",
    "positive, negative = checkDatasetBalance(RetinaDataset(retina_array)) # Without transform\n",
    "ratio_n_p = negative / positive\n",
    "print('Total Negative Labels: ' + str(negative))\n",
    "print('Total Positive Labels: ' + str(positive))\n",
    "print('Negative labels for each positive: ' + str(ratio_n_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting Explanation\n",
    "In an unbalanced dataset, the class with more examples contributes more to the loss. By weighting the classes we guarantee that all classes contribute equally to the loss. This improves performance because it forces the network to learn all classes well to improve the loss. In an unbalanced dataset, without weighting applied, the network might learn information about the frequency of the classes as part of its predication reducing the ability of the network to generalize well. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3 (45 points)\n",
    "Build a u-net model class that inherits from the torch.nn.Module class. Follow the model as described in the original paper (https://arxiv.org/pdf/1505.04597.pdf - Fig. 1 and Section 2), but with these modifications chosen to simplify the assignment and to reduce memory use:\n",
    "- Add 2D batch normalizations between convolutions and ReLUs.\n",
    "- Add paddings to the convolutions so that the outputs of the convolutions have the same spatial size as the inputs. Because of this, the cropping before the concatenation of the skip connections is not necessary.\n",
    "- The upsampling convolutions should be coded using the layer torch.nn.ConvTransposed2D. More details to understand what they meant in the paper can be found in the video here, starting at 2:22:  https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/\n",
    "- Reduce the number of channels of all internal layers dividing them by 8. \n",
    "- Input should still have 3 channels, but output should have only one channel (binary output). \n",
    "- You can use PyTorch's weight initialization. You do not need to implement the initialization of weights as described in the u-net paper.\n",
    "\n",
    "Use the masks provided with the dataset to mask your loss. Your loss should only backpropagate through the pixels where the mask has a value of 1, and not backpropagate where the mask is 0. Hyperparameters and methods to use are provided in the code cell below. Use the learning rate scheduler for training too. Your network should be able to get an F1 score of at least 0.75 in the validation set of the provided dataset, using the ``get_score_model`` function. Test your model on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be reused in U-net\n",
    "class conv3block(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(conv3block, self).__init__()\n",
    "        self.conv = torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1)\n",
    "        self.batch = torch.nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.batch(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class convTran2d(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(convTran2d, self).__init__()\n",
    "        self.convT = torch.nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, stride=2, kernel_size=4, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.convT(x)\n",
    "        return x\n",
    "     \n",
    "        \n",
    "class UNet(torch.nn.Module):\n",
    "    def __init__(self, convBlock, convTranspose, deactivate_skip=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.deactivate_skip = deactivate_skip\n",
    "        # left side\n",
    "        self.conv8_1 = convBlock(in_channels=3, out_channels=8)\n",
    "        self.conv8_2 = convBlock(in_channels=8, out_channels=8)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.conv16_1 = convBlock(in_channels=8, out_channels=16)\n",
    "        self.conv16_2 = convBlock(in_channels=16, out_channels=16)\n",
    "        # maxpool\n",
    "        self.conv32_1 = convBlock(in_channels=16, out_channels=32)\n",
    "        self.conv32_2 = convBlock(in_channels=32, out_channels=32)\n",
    "        # maxpool\n",
    "        self.conv64_1 = convBlock(in_channels=32, out_channels=64)\n",
    "        self.conv64_2 = convBlock(in_channels=64, out_channels=64)\n",
    "        # maxpool\n",
    "        self.conv128_1 = convBlock(in_channels=64, out_channels=128)\n",
    "        self.conv128_2 = convBlock(in_channels=128, out_channels=128)\n",
    "        \n",
    "        # right side\n",
    "        out_c = 128 if self.deactivate_skip else 64\n",
    "        self.convT_1 = convTranspose(in_channels=128, out_channels=out_c)\n",
    "        self.conv64_3 = convBlock(in_channels=128, out_channels=64)\n",
    "        self.conv64_4 = convBlock(in_channels=64, out_channels=64)\n",
    "        \n",
    "        out_c = 64 if self.deactivate_skip else 32 \n",
    "        self.convT_2 = convTranspose(in_channels=64, out_channels=out_c)\n",
    "        self.conv32_3 = convBlock(in_channels=64, out_channels=32)\n",
    "        self.conv32_4 = convBlock(in_channels=32, out_channels=32)\n",
    "        \n",
    "        out_c = 32 if self.deactivate_skip else 16 \n",
    "        self.convT_3 = convTranspose(in_channels=32, out_channels=out_c)\n",
    "        self.conv16_3 = convBlock(in_channels=32, out_channels=16)\n",
    "        self.conv16_4 = convBlock(in_channels=16, out_channels=16)\n",
    "        \n",
    "        out_c = 16 if self.deactivate_skip else 8 \n",
    "        self.convT_4 = convTranspose(in_channels=16, out_channels=out_c)\n",
    "        self.conv8_3 = convBlock(in_channels=16, out_channels=8)\n",
    "        self.conv8_4 = convBlock(in_channels=8, out_channels=8)\n",
    "        self.conv1 = convBlock(in_channels=8, out_channels=1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # left side\n",
    "        x = self.conv8_1(x)\n",
    "        skip_1 = self.conv8_2(x)\n",
    "        x = self.pool(skip_1)\n",
    "        \n",
    "        x = self.conv16_1(x)\n",
    "        skip_2 = self.conv16_2(x)\n",
    "        x = self.pool(skip_2)\n",
    "        \n",
    "        x = self.conv32_1(x)\n",
    "        skip_3 = self.conv32_2(x)\n",
    "        x = self.pool(skip_3)\n",
    "        \n",
    "        x = self.conv64_1(x)\n",
    "        skip_4 = self.conv64_2(x)\n",
    "        x = self.pool(skip_4)\n",
    "        \n",
    "        x = self.conv128_1(x)\n",
    "        x = self.conv128_2(x)\n",
    "        \n",
    "        # right side\n",
    "        x = self.convT_1(x)\n",
    "        x = x if self.deactivate_skip else torch.cat((x, skip_4), dim=1) \n",
    "        x = self.conv64_3(x)\n",
    "        x = self.conv64_4(x)\n",
    "        \n",
    "        x = self.convT_2(x)\n",
    "        x = x if self.deactivate_skip else torch.cat((x, skip_3), dim=1) \n",
    "        x = self.conv32_3(x)\n",
    "        x = self.conv32_4(x)\n",
    "        \n",
    "        x = self.convT_3(x)\n",
    "        x = x if self.deactivate_skip else torch.cat((x, skip_2), dim=1) \n",
    "        x = self.conv16_3(x)\n",
    "        x = self.conv16_4(x)\n",
    "        \n",
    "        x = self.convT_4(x)\n",
    "        x = x if self.deactivate_skip else torch.cat((x, skip_1), dim=1) \n",
    "        x = self.conv8_3(x)\n",
    "        x = self.conv8_4(x)\n",
    "        x = self.conv1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=6, shuffle=True, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "#instantiate your model here:\n",
    "model = UNet(conv3block, convTran2d)\n",
    "model = model.cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "n_epochs = 200\n",
    "# Learning rate is reduced after plateauing to stabilize the end of training.\n",
    "# use the learning rate scheduler as defined here. Example on how to integrate it to training in\n",
    "# https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.StepLR\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)\n",
    "\n",
    "#train your model here:\n",
    "# Define loss\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor(ratio_n_p))\n",
    "\n",
    "val_scores = []\n",
    "train_scores = []\n",
    "target_score = 0.75\n",
    "best_score = 0\n",
    "best_model = copy.deepcopy(model)\n",
    "\n",
    "print('Starting Training!')\n",
    "for epoch in range(n_epochs):\n",
    "    print('\\nEpoch: ' + str(epoch))\n",
    "    scheduler.step()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    losses = []\n",
    "    for image, segmentation, mask in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        image = image.cuda()\n",
    "        segmentation = segmentation.cuda()\n",
    "        mask = mask.cuda()\n",
    "        out = model(image)\n",
    "        loss = criterion(torch.mul(out, mask),  torch.mul(segmentation, mask))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    print('Loss: ' + str(np.mean(losses)))\n",
    "    \n",
    "    # Get training data accuracy\n",
    "    train_score = get_score_model(model, train_loader)\n",
    "    mean_train_score = np.mean(train_score)\n",
    "    print('Trainig Score: ' + str(mean_train_score))\n",
    "    train_scores.append(mean_train_score)\n",
    "    \n",
    "    # Get validation data accuracy\n",
    "    val_score = get_score_model(model, val_loader)\n",
    "    mean_val_score = np.mean(val_score)\n",
    "    print('Validation Score: ' + str(mean_val_score))\n",
    "    val_scores.append(mean_val_score)\n",
    "    \n",
    "    # Save Best Model\n",
    "    if mean_val_score > target_score and mean_val_score > best_score:\n",
    "        best_score = mean_val_score\n",
    "        best_model = copy.deepcopy(model)\n",
    "        print('New Best Model Saved!')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training and Validation F1 Score vs Epoch Number\n",
    "plt.plot(range(n_epochs), train_scores, 'b', range(n_epochs), val_scores, 'r')\n",
    "plt.title('F1 Scores for UNet Model')\n",
    "plt.legend(['Training Data', 'Validation Data'])\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNet on Test Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = get_score_model(best_model, test_loader)\n",
    "print('F1 score for test data: ' + str(test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4 (8 points)\n",
    "Visualize a few outputs of your network in the validation set and compare with your ground truth. Comment on the kinds of mistakes that you are able to distinguish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets example outputs\n",
    "def get_example_outputs(model, data_loader):\n",
    "    #toggle model to eval mode\n",
    "    model.eval()\n",
    "    ground_truth = []\n",
    "    model_output = []\n",
    "    with torch.no_grad():\n",
    "        logits_predicted = np.zeros([0, 1, 304, 352])\n",
    "        segmentations = np.zeros([0, 1, 304, 352])\n",
    "\n",
    "        for image, segmentation, mask  in data_loader:\n",
    "            image = image.cuda()\n",
    "            model_output.append(model(image))\n",
    "            ground_truth.append(segmentation)\n",
    "            if len(model_output) == 5:\n",
    "                return model_output, ground_truth;\n",
    "            \n",
    "\n",
    "model_out, gt = get_example_outputs(model, val_loader)\n",
    "for idx in range(5):\n",
    "    plt.figure()\n",
    "    plt.title(\"Model Output\")\n",
    "    model_out[idx] = model_out[idx].view(model_out[idx].shape[2], model_out[idx].shape[3])\n",
    "    plt.imshow(model_out[idx].cpu().numpy())\n",
    "#     plt.savefig('img/model'+str(idx)+'.png') # Saved image so I could get better look\n",
    "    plt.figure()\n",
    "    plt.title(\"Segmentation ground truth\")\n",
    "    gt[idx] = gt[idx].view(gt[idx].shape[2], gt[idx].shape[3])\n",
    "    plt.imshow(gt[idx].cpu().numpy())\n",
    "#     plt.savefig('img/gt'+str(idx)+'.png') # Saved image so I could get better look\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistakes I can Identify\n",
    "In the model output the segmentation lines are much thinner than the ground truth. When the ground truth has disjoint regions or faint lines the output does not have those lines or they are extremely faint or fuzzy and hard to see or identify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5 (23 points)\n",
    "Modify your u-net module to receive a boolean argument in initialization/contruction that can deactivate the use of the skip connections. To compensate for not having the extra channels coming from the skip connection, you should double the number of channels out of the upsampling layer when skip connections are deactivated. Train and test the network without the skip connections, using the hyperparameters as provided in the cell below. You should be able to get an F1 score of at least 0.5.  When compared to the network trained on Question 3, which network performs better? Explain why that happens, detailing the role of the skip connections for a segmentation task. Link your explanations to visualizations of the output of the network without skip connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=6, shuffle=True, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "#instantiate your model here:\n",
    "model = UNet(conv3block, convTran2d, deactivate_skip=True)\n",
    "model = model.cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "n_epochs = 200\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)\n",
    "\n",
    "#train your model here:\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor(ratio_n_p))\n",
    "\n",
    "val_scores = []\n",
    "train_scores = []\n",
    "target_score = 0.50\n",
    "best_score = 0\n",
    "best_model = copy.deepcopy(model)\n",
    "\n",
    "print('Starting Training!')\n",
    "for epoch in range(n_epochs):\n",
    "    print('\\nEpoch: ' + str(epoch))\n",
    "    scheduler.step()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    losses = []\n",
    "    for image, segmentation, mask in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        image = image.cuda()\n",
    "        segmentation = segmentation.cuda()\n",
    "        mask = mask.cuda()\n",
    "        out = model(image)\n",
    "        loss = criterion(torch.mul(out, mask),  torch.mul(segmentation, mask))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    print('Loss: ' + str(np.mean(losses)))\n",
    "    \n",
    "    # Get training data accuracy\n",
    "    train_score = get_score_model(model, train_loader)\n",
    "    mean_train_score = np.mean(train_score)\n",
    "    print('Trainig Score: ' + str(mean_train_score))\n",
    "    train_scores.append(mean_train_score)\n",
    "    \n",
    "    # Get validation data accuracy\n",
    "    val_score = get_score_model(model, val_loader)\n",
    "    mean_val_score = np.mean(val_score)\n",
    "    print('Validation Score: ' + str(mean_val_score))\n",
    "    val_scores.append(mean_val_score)\n",
    "    \n",
    "    # Save Best Model\n",
    "    if mean_val_score > target_score and mean_val_score > best_score:\n",
    "        best_score = mean_val_score\n",
    "        best_model = copy.deepcopy(model)\n",
    "        print('New Best Model Saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training and Validation F1 Score vs Epoch Number\n",
    "plt.plot(range(n_epochs), train_scores, 'b', range(n_epochs), val_scores, 'r')\n",
    "plt.title('F1 Scores for UNet Model Without Skip Connections')\n",
    "plt.legend(['Training Data', 'Validation Data'])\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNet Without Skip on Test Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = get_score_model(best_model, test_loader)\n",
    "print('F1 score for test data: ' + str(test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets example outputs\n",
    "def get_example_outputs(model, data_loader):\n",
    "    #toggle model to eval mode\n",
    "    model.eval()\n",
    "    ground_truth = []\n",
    "    model_output = []\n",
    "    with torch.no_grad():\n",
    "        for image, segmentation, mask  in data_loader:\n",
    "            image = image.cuda()\n",
    "            model_output.append(model(image))\n",
    "            ground_truth.append(segmentation)\n",
    "            if len(model_output) == 5:\n",
    "                return model_output, ground_truth\n",
    "            \n",
    "\n",
    "model_out, gt = get_example_outputs(model, val_loader)\n",
    "for idx in range(5):\n",
    "    plt.figure()\n",
    "    plt.title(\"Model Output\")\n",
    "    model_out[idx] = model_out[idx].view(model_out[idx].shape[2], model_out[idx].shape[3])\n",
    "    plt.imshow(model_out[idx].cpu().numpy())\n",
    "#     plt.savefig('img/model_skip'+str(idx)+'.png') # Saved image so I could get better look\n",
    "    plt.figure()\n",
    "    plt.title(\"Segmentation ground truth\")\n",
    "    gt[idx] = gt[idx].view(gt[idx].shape[2], gt[idx].shape[3])\n",
    "    plt.imshow(gt[idx].cpu().numpy())\n",
    "#     plt.savefig('img/gt_skip'+str(idx)+'.png') # Saved image so I could get better look\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation \n",
    "The network in Q3 performed better. \n",
    "\n",
    "In the paper [1] the skip connections are explained as improving localization by combining the locality information from the downsampling path with the contextual information in the upsampling path resulting in a more precise output. \n",
    "\n",
    "Looking at the model outputs from Q5 we see that the segmentations are considerably blurrier than the segmentations from Q3. Witout the locality information from the skip connection the network seems more uncertain about where the positive lables should be in the image; thus the blurrines.\n",
    "\n",
    "[1] Ronneberger, O., Fischer, P., & Brox, T. (2015). U-net: Convolutional networks for biomedical image segmentation. In Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) (Vol. 9351, pp. 234â€“241). Springer Verlag. https://doi.org/10.1007/978-3-319-24574-4_28"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dir",
   "language": "python",
   "name": "env_dir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
