{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "You are expected to complete this notebook with lines of code, plots and texts. Sometimes you will make modifications to existing cells and sometimes you may need to create new cells with original code or text for your analyses. This assignment has a total of 100 points.\n",
    "\n",
    " \n",
    "These are the kinds of questions you will have:\n",
    "- Questions that require only modification of code will be marked with a **C**\n",
    "- Theoretical questions that require only text or mathematical equations to answer will be marked with a **T**. When an equation is asked, you can write it in python/numpy syntax (enclose it using \\`\\`) or in latex syntax (enclose it using \\$\\$).\n",
    "- Questions that require an analysis of results that may include text, code and visualizations will be marked with an **A**. The first questions of this type will be more explicit of what you should write. They will become more open-ended as the assignment goes on.\n",
    "\n",
    "Using Canvas, you will deliver the notebook file (.ipynb) with cells executed and outputs visible.\n",
    "\n",
    "- No other packages than the ones already imported can be used.\n",
    "- No other data than the variables provided should be used.\n",
    "- The cell outputs present in your delivered notebook should be reproducible by us by running your notebook cells in order.\n",
    "- Your final delivery should not have modified current code outside of these demarcations, but, if needed for testing, feel free to modify it and later reverse to its original state.\n",
    "- All code must be your own work. Code cannot be copied from another source or student. You may copy code from cells that were pre-defined in this notebook if you think it is useful for use in another question.\n",
    "- All images must be generated from data generated in your code. Do NOT import/display images generated outside your code.\n",
    "- Your analysis must be your own, but if you quote text or equations from another source make sure to cite the reference.\n",
    "\n",
    "Other notes:\n",
    "- Cells should be run in order, using Shift+Enter.\n",
    "- Read all the provided code cells and its comments, as it contains variables and information that you may need to use to complete the notebook.\n",
    "- Existing cells that require your input with code will be marked with comments `##your code starts here` and `##your code ends here` to specify where you need to write code.\n",
    "- Ends of questions are marked with **END**, to try to make it clearer where your modifications to the notebook should be provided. \n",
    "- To create a text cell, create it with the \"+\" button and change its type from \"Code\" to \"Markdown\" using the upper menu. To modify a text cell, double click on it.\n",
    "- If you are interested, you can check detail on formatting markdown text here: https://medium.com/ibm-data-science-experience/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed\n",
    "- A python/numpy/matplotlib tutorial that you might find useful: http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting libraries and useful functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/07/16d781df15be30df4acfd536c479268f1208b2dfbc91e9ca5d92c9caf673/matplotlib-3.0.2-cp36-cp36m-manylinux1_x86_64.whl (12.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 12.9MB 718kB/s  eta 0:00:01MB 48.0MB/s eta 0:00:01��                 | 5.8MB 119.7MB/s eta 0:00:01��█████▉     | 10.8MB 110.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/0a/001be530836743d8be6c2d85069f46fecf84ac6c18c7f5fb8125ee11d854/pyparsing-2.3.1-py2.py3-none-any.whl (61kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 3.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/a7/88719d132b18300b4369fbffa741841cfd36d1e637e1990f27929945b538/kiwisolver-1.0.1-cp36-cp36m-manylinux1_x86_64.whl (949kB)\n",
      "\u001b[K    100% |████████████████████████████████| 952kB 2.0MB/s ta 0:00:0111\n",
      "\u001b[?25hCollecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.10.0 in ./env_dir/lib/python3.6/site-packages (from matplotlib) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in ./env_dir/lib/python3.6/site-packages (from matplotlib) (2.7.5)\n",
      "Requirement already satisfied: setuptools in ./env_dir/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (40.7.1)\n",
      "Requirement already satisfied: six in ./env_dir/lib/python3.6/site-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
      "Installing collected packages: pyparsing, kiwisolver, cycler, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.0.1 matplotlib-3.0.2 pyparsing-2.3.1\n",
      "Requirement already satisfied: numpy in ./env_dir/lib/python3.6/site-packages (1.16.0)\n",
      "Collecting pandas\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/e1/4a63ed31e1b1362d40ce845a5735c717a959bda992669468dae3420af2cd/pandas-0.24.0-cp36-cp36m-manylinux1_x86_64.whl (10.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 10.1MB 821kB/s eta 0:00:011███████▍           | 6.4MB 119.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.0 in ./env_dir/lib/python3.6/site-packages (from pandas) (2.7.5)\n",
      "Collecting pytz>=2011k (from pandas)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/28/1d3920e4d1d50b19bc5d24398a7cd85cc7b9a75a490570d5a30c57622d34/pytz-2018.9-py2.py3-none-any.whl (510kB)\n",
      "\u001b[K    100% |████████████████████████████████| 512kB 4.4MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in ./env_dir/lib/python3.6/site-packages (from pandas) (1.16.0)\n",
      "Requirement already satisfied: six>=1.5 in ./env_dir/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-0.24.0 pytz-2018.9\n",
      "Collecting python-mnist\n",
      "  Downloading https://files.pythonhosted.org/packages/05/9c/f1c1e4d011b01ac436bba0ac6715b3f988bb7f8fec6f21f89cf820aa33e1/python-mnist-0.6.tar.gz\n",
      "Building wheels for collected packages: python-mnist\n",
      "  Building wheel for python-mnist (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/u0579755/.cache/pip/wheels/28/29/36/408f83545a511c43d03ef997a1dc99b49ccd5f9f306ed92468\n",
      "Successfully built python-mnist\n",
      "Installing collected packages: python-mnist\n",
      "Successfully installed python-mnist-0.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'divide': 'raise', 'over': 'raise', 'under': 'raise', 'invalid': 'raise'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#install a few libraries that will be used\n",
    "!pip3 install matplotlib\n",
    "!pip3 install numpy\n",
    "!pip3 install pandas\n",
    "!pip3 install python-mnist\n",
    "\n",
    "#import a few libraries. Numpy is named as np and pyplot in matplotlib as plt\n",
    "import urllib.request\n",
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from utils import test_gradient, preprocess_medical_data, load_and_preprocess_mnist\n",
    "\n",
    "#needed to plot plots with matplotlib in OSX\n",
    "%matplotlib inline\n",
    "\n",
    "#set numpy to raise exceptions when encountering numerical errors\n",
    "np.seterr(all='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is used to convert from integer encoding of labels to one hot encoding\n",
    "# labels is an 1-D array with the integer labels from 0 to n_labels. \n",
    "def one_hot(labels, n_labels):\n",
    "    return np.squeeze(np.eye(n_labels)[labels.reshape(-1)])\n",
    "\n",
    "#Does the transpose of the last two axes of a tensor\n",
    "def T(input_tensor):\n",
    "    return np.swapaxes(input_tensor, -1, -2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Analyzing model capacity in a polynomial toy example\n",
    "**(Total of 26 points)**\n",
    "\n",
    "This exercise will illustrate how validation error of a model evolves by changing model capacity. We are going to start with a simple example of generated data that follows a third degree polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C 1.1 (3 points)**: Write the body of the function `third_degree_polynomial` that returns the output of a third degree polynomial. The function receives two numpy arrays, `x` and `constants_vector`. `x` will have the shape [batch, 1] and `constants_vector` [4,1]. If `constants_vector` is $[a_0, a_1, a_2, a_3]$, the function should return, for each of the batch examples indexed by i, $a_3\\times x_i^3 + a_2\\times x_i^2 + a_1\\times x_i + a_0$. The shape of the returned array should be [batch, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def third_degree_polynomial(x, constants_vector):\n",
    "    ##your code starts here\n",
    "    x_2 = np.multiply(x, x);\n",
    "    x_3 = np.multiply(x_2, x);\n",
    "    return constants_vector[0] + np.multiply(constants_vector[1], x) + np.multiply(constants_vector[2], x_2) + np.multiply(constants_vector[3], x_3)\n",
    "    ##your code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. ]\n",
      " [0.1]\n",
      " [0.2]\n",
      " [0.3]\n",
      " [0.4]\n",
      " [0.5]\n",
      " [0.6]\n",
      " [0.7]\n",
      " [0.8]\n",
      " [0.9]\n",
      " [1. ]\n",
      " [1.1]\n",
      " [1.2]\n",
      " [1.3]\n",
      " [1.4]\n",
      " [1.5]\n",
      " [1.6]\n",
      " [1.7]\n",
      " [1.8]\n",
      " [1.9]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f3b7c11b7499>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#test your function to make sure it is doing what is expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpredicted_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthird_degree_polynomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2.4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.52\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5.3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5.54652\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5.81136\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m6.10244\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m6.42768\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m6.795\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;36m7.21232\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;36m7.68756\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;36m8.22864\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;36m8.84348\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;36m9.54\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10.32612\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m11.20976\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m12.19884\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m13.30128\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m14.525\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m15.87792\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m17.36796\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m19.00304\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m20.79108\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Your third_degree_polynomial function seems to be returning the expected values'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env_dir/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mallclose\u001b[0;34m(a, b, rtol, atol, equal_nan)\u001b[0m\n\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m     \"\"\"\n\u001b[0;32m-> 2423\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0matol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mequal_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mequal_nan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2424\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env_dir/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36misclose\u001b[0;34m(a, b, rtol, atol, equal_nan)\u001b[0m\n\u001b[1;32m   2519\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2521\u001b[0;31m     \u001b[0mxfin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2522\u001b[0m     \u001b[0myfin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2523\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxfin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "#test your function to make sure it is doing what is expected\n",
    "predicted_values = third_degree_polynomial(np.expand_dims(np.arange(0,2,0.1), axis = 1), np.expand_dims(np.array([5.3,2.4,0.52,1.32]), axis = 1))\n",
    "if np.allclose(predicted_values, np.array([[5.3], [5.54652], [5.81136], [6.10244], [6.42768], [6.795], [ 7.21232], [ 7.68756], [ 8.22864], [ 8.84348], [ 9.54], [10.32612], [11.20976], [12.19884], [13.30128], [14.525], [15.87792], [17.36796], [19.00304], [20.79108]])):\n",
    "    print('Your third_degree_polynomial function seems to be returning the expected values')\n",
    "else:\n",
    "    print('WARNING: Your third_degree_polynomial function is not returning the expected values. You should review your code.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END C 1.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting data for Exercise 1 and 2. Pay attention to the names of the variables because you are going to need to use them in parts of your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Generating input data and target data for this exercise\n",
    "#noise is added to data to make the fitting inexact\n",
    "\n",
    "#Training data input you are going to use for exercises 1 and 2\n",
    "x_ex1_train = np.expand_dims(np.arange(0,2,0.1), axis = 1)\n",
    "\n",
    "#coefficients of the third degree polynomial used to generate the data\n",
    "coefficients_ex1 = np.array([[1,-2,3,-1]]).T\n",
    "\n",
    "y_ex1_train = third_degree_polynomial(x_ex1_train, coefficients_ex1) \n",
    "\n",
    "#Testing data you are going to use for exercises 1 and 2\n",
    "x_ex1_val = np.expand_dims(np.arange(0,2,0.01), axis = 1)\n",
    "y_ex1_val = third_degree_polynomial(x_ex1_val,coefficients_ex1) \n",
    "\n",
    "#add noise to it\n",
    "np.random.seed(1)\n",
    "y_ex1_train = y_ex1_train + 0.2 * np.random.normal(size = x_ex1_train.shape)\n",
    "y_ex1_val = y_ex1_val + 0.2 * np.random.normal(size = x_ex1_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the generated data to visualize what we just generated for this exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the data we just generated as a sanity check\n",
    "plt.plot(x_ex1_train, y_ex1_train);\n",
    "plt.title('Noisy 3rd Degree Polynomial');\n",
    "plt.xlabel('Input');\n",
    "plt.ylabel('Ground truth');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we are going to fit polynomial functions to this data that we just generated. Since we are going to be using polynomials, we can use a closed solution fitting function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C 1.2 (3 points)**: Write the remaining body of the function `fit`, that receives the degree of the polynomial to fit (an integer), and input and target arrays (both of shape [batch, 1]) and that returns the coefficients of that polynomial (of shape [degree, 1]) with the least mean square error for the provided data (polynomial least square fit). The only part left for you to complete is the building of the Design Matrix `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(degree, inputs, targets):\n",
    "    ##your code starts here\n",
    "    \n",
    "    ##your code ends here\n",
    "    return np.linalg.inv(X.T@X)@X.T@targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test your function to make sure it is doing what is expected\n",
    "weights_ex1_train = fit(5,x_ex1_train,y_ex1_train)\n",
    "if np.allclose(weights_ex1_train, np.array([[  1.2574006 ], [ -5.50854483], [ 13.21760281], [-12.60429544], [  5.62135688], [ -0.97197598]])):\n",
    "    print('Your fitting function seems to be returning the expected values')\n",
    "else:\n",
    "    print('WARNING: Your fitting function is not returning the expected values. You should review your code.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END C 1.2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C 1.3 (4 points)**: Since we are going to have coefficients for polynomials of different degrees, we should also be able to get their evaluated values for different inputs. Write the body of the `any_degree_polynomial` function, which receives two numpy arrays, `x` and `constants_vector`. `x` will have the shape [batch, 1] and `constants_vector` [degree,1]. If `constants_vector` is $[a_0, a_1, a_2]$, for example, the function should return, for each of the batch examples indexed by i, $a_2\\times x_i^2 + a_1\\times x_i + a_0$. The shape of the returned array should be [batch, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def any_degree_polynomial(x, constants_vector):\n",
    "    ##your code starts here\n",
    "    \n",
    "    ##your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END C 1.3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C 1.4 (3 points)**: Fit a third degree polynomial to the provided data and plot, in the same graph, your fitted results and the training data. Use a legend to identify what each curve is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Fit a 3rd degree polynomial to data and visualize the result of your fit\n",
    "##your code starts here\n",
    "\n",
    "##your code starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END C 1.4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to evaluate how polynomials of different degrees perform in the validation data when fitted to the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C 1.5 (3 points)**: First we are going to define the function that returns the metric used to evaluate the results. Complete the body of the `mse` function. The function receives two arrays of same shape, [batch, 1], and returns the mean square error, defined by  $\\frac{1}{N}\\sum{\\left( \\hat{y} - y \\right)^{2}}$, between them. The returned value should be a float variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(predicted_values, targets):\n",
    "    ##your code starts here\n",
    "    \n",
    "    ##your code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test your function to make sure it is doing what is expected\n",
    "mse_testing_function = mse(y_ex1_val[:20,:],y_ex1_train)\n",
    "if np.allclose(np.array([mse_testing_function]), np.array([0.23499996035092258])):\n",
    "    print('Your error function seems to be returning the expected values')\n",
    "else:\n",
    "    print('WARNING: Your error function is not returning the expected values. You should review your code.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END C 1.5**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A 1.6 (10 points)**: We are going to treat the degree of the fitting polynomial as the hyperparameter of our model. Fit polynomials of degrees from 1 to 10, plot the mean square error for each different degree using, in separate curves but same graph, both the training data and the validation data. Write a short analysis of the results presented in your plot, stating which degrees are overfitting and which are underfitting, and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##your code starts here\n",
    "\n",
    "##your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END A 1.6**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Defining and training fully connected networks\n",
    "**(Total of 31 points)**\n",
    "\n",
    "In this exercise, we are going to define functions to train a fully connected network with one hidden layer. First, we set a function to initialize the learnable parameters of the network. They are going to be bidimensional arrays stored in a python dictionary, where the keys of the dictionary represent the name of the parameters of the network. The parameters are called 'weights_i' and 'bias_i' where i is the layer where the parameter is used. Instead of using the equation $W^Tx+b$ for linear layers as seen in class, use $vW+b$, $v$ being a row-vector. We do so for practical reasons of facilitating batch vector-matrix multiplication in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_ex2(n_inputs, n_hidden_nodes, n_outputs):\n",
    "    np.random.seed(1)\n",
    "    #initialize weights centered in 0\n",
    "    weights_1 = np.random.normal(0,0.5,[n_inputs,n_hidden_nodes])\n",
    "    # initialize bias with a small positive value to reduce amount of dead neurons\n",
    "    bias_1 = np.random.normal(0.1,0,[n_hidden_nodes])\n",
    "    #initialize weights centered in 0\n",
    "    weights_2 = np.random.normal(0,0.5,[n_hidden_nodes,n_outputs])\n",
    "    # initialize bias with a small positive value to reduce amount of dead neurons\n",
    "    bias_2 = np.random.normal(0.1,0,[n_outputs])\n",
    "    return {'weights_1':weights_1, 'weights_2':weights_2, 'bias_1':bias_1, 'bias_2':bias_2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C 2.1 (6 points)**: Write the body of the function `two_layer_network_forward` that defines a forward pass of a two-layer fully connected network with ReLU as activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_network_forward(inputs, parameters, return_intermediary_results = False):\n",
    "    \n",
    "    ##your code starts here\n",
    "    out_1 =  #output of the first linear layer before activation function; it should have shape [batch, n_hidden_nodes] \n",
    "    out_1_relu =   #output of the first activation function; it should have shape [batch, n_hidden_nodes] \n",
    "    out_2 =  #output of the second linear_layer; it should have shape [batch, n_outputs] \n",
    "    ##your code ends here\n",
    "    \n",
    "    #return_intermediary_results should only be True if you are going to use this forward pass\n",
    "    # to calculate gradients for the network parameters\n",
    "    if return_intermediary_results:\n",
    "        #if you are doing the forward pass to calculate backward pass afterwards, you are going to need all intermediary results\n",
    "        to_return = {'out_1': out_1, 'out_1_relu': out_1_relu, 'out_2': out_2}\n",
    "    else:\n",
    "        #if you are doing the forward pass just to get the output of the network, you only need the final result\n",
    "        to_return = out_2\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test your function\n",
    "predicted_outputs_testing_function = two_layer_network_forward(x_ex1_train, initialize_parameters_ex2(1, 20, 1))\n",
    "if np.allclose(predicted_outputs_testing_function, np.array([[0.30413895], [0.28105381], [0.21761113], [0.236233  ], [0.27359281], [0.2995363 ], [0.32282194], [0.33807287], [0.35145301], [0.36860323], [0.38583117], [0.40305912], [0.42173721], [0.44258177], [0.46342632], [0.48427088], [0.50511543], [0.52595998], [0.54680454], [0.56764909]])):    \n",
    "    print('Your forward function seems to be returning the expected values')\n",
    "else:\n",
    "    print('WARNING: Your forward function is not returning the expected values. You should review your code.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END C 2.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**T 2.2 (3 points)**: Calculate how many parameters a network initialized with initialize_parameters_ex2 would have, as a function of n_inputs, n_hidden_nodes and n_outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Put your final answer here:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END T 2.2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C 2.3 (14 points)**: Fill in the code for each of the required derivatives. \n",
    "\n",
    "Tips: for one input example, if $L=f(y)$ and $y = vW+b$, \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W}  = \\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial W}  = v^T\\frac{\\partial L}{\\partial y}$$ \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial v}  = \\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial v}  = \\frac{\\partial L}{\\partial y}W^T$$ \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b}=\\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial b}  = \\frac{\\partial L}{\\partial y}$$ \n",
    "\n",
    "$L$ being a scalar, $v$ a row-vector, $W$ a matrix and $b$ a row-vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss_backward(predicted, gt):\n",
    "    ##your code starts here\n",
    "    derivative_of_mse_loss_with_respect_to_predicted =  #d(loss)/d(predicted); it should have shape [batch, n_outputs] \n",
    "    ##your code ends here\n",
    "    return derivative_of_mse_loss_with_respect_to_predicted\n",
    "\n",
    "def two_layer_network_backward(inputs, parameters, gt, loss_backward):\n",
    "    \n",
    "    intermediary_results_in_forward = two_layer_network_forward(inputs, parameters, return_intermediary_results = True)\n",
    "\n",
    "    out_1 = intermediary_results_in_forward['out_1'] \n",
    "    out_1_relu = intermediary_results_in_forward['out_1_relu'] \n",
    "    out_2 = intermediary_results_in_forward['out_2'] \n",
    "    \n",
    "    derivative_of_loss_with_respect_to_out_2 =  loss_backward(out_2, gt) \n",
    "    \n",
    "    ##your code starts here\n",
    "    \n",
    "    derivative_of_loss_with_respect_to_bias_2 =  #d(loss)/d(bias_2); it should have shape [batch, n_outputs] \n",
    "    derivative_of_loss_with_respect_to_weights_2 =  #d(loss)/d(weights_2); it should have shape [batch, n_hidden_nodes, n_outputs] \n",
    "    derivative_of_loss_with_respect_to_out_1_relu =  #d(loss)/d(out_1_relu); it should have shape [batch, n_hidden_nodes] \n",
    "    derivative_of_loss_with_respect_to_out_1 =  #d(loss)/d(out_1); it should have shape [batch, n_hidden_nodes] \n",
    "    derivative_of_loss_with_respect_to_bias_1 =  #d(loss)/d(bias_1); it should have shape [batch, n_hidden_nodes] \n",
    "    derivative_of_loss_with_respect_to_weights_1 =  #d(loss)/d(weights_1); it should have shape [batch, n_inputs, n_hidden_nodes] \n",
    "    \n",
    "    ##your code ends here\n",
    "    \n",
    "    return {\n",
    "            'weights_1': np.mean(derivative_of_loss_with_respect_to_weights_1, axis = 0),\n",
    "            'bias_1': np.mean(derivative_of_loss_with_respect_to_bias_1, axis = 0), \n",
    "            'weights_2':np.mean(derivative_of_loss_with_respect_to_weights_2, axis = 0) , \n",
    "            'bias_2':np.mean(derivative_of_loss_with_respect_to_bias_2, axis = 0)\n",
    "            }\n",
    "\n",
    "def two_layer_network_mse_backward(inputs, parameters, gt):\n",
    "    return two_layer_network_backward(inputs, parameters, gt, mse_loss_backward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gradient(two_layer_network_forward, two_layer_network_mse_backward, mse, x_ex1_train[0:20,:], y_ex1_train[0:20,:], initialize_parameters_ex2(1, 10, 1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END C 2.3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C 2.4 (8 points)** Write the update rule for a batch of training. Your function `run_batch_sgd` should calculate the gradients and update the parameters using the vanilla stochastic gradient descent update rule. You should also implement the use of this function for each batch in the second block of your code and the plot of the results in the last block of your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_batch_sgd(backward_function, parameters, learning_rate, inputs, targets):\n",
    "    #calculate gradients and update parameters using sgd update rule\n",
    "    ##your code starts here\n",
    "    \n",
    "    ##your code ends here\n",
    "    return updated_parameters\n",
    "\n",
    "n_hidden_nodes = 50\n",
    "parameters_two_layer_regression = initialize_parameters_ex2(1, n_hidden_nodes, 1)\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 20\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    shuffled_indexes = (np.arange(x_ex1_train.shape[0]))\n",
    "    np.random.shuffle(shuffled_indexes)\n",
    "    shuffled_indexes = shuffled_indexes.reshape([-1, batch_size])\n",
    "    for batch_i in range(shuffled_indexes.shape[1]):    \n",
    "        batch = shuffled_indexes[:,batch_i]\n",
    "        input_this_batch = x_ex1_train[batch,:]\n",
    "        gt_this_batch =  y_ex1_train[batch,:]\n",
    "        #use you function run_batch_sgd to update the parameters\n",
    "        ##your code starts here\n",
    "        \n",
    "        ##your code ends here\n",
    "\n",
    "#plot the results of training\n",
    "##your code starts here\n",
    "\n",
    "##your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END C 2.4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Medical Dataset - Classification task with cross-entropy and softmax\n",
    "**(Total of 19 points)**\n",
    "\n",
    "In this exercise we are going to complement what was defined in Exercise 2, to make it compatible with classification tasks, and analyze hyperparameters. The dataset we are going to use here is a medical dataset, available at https://archive.ics.uci.edu/ml/datasets/Epileptic+Seizure+Recognition, that contains preprocessed 1-D signals of EEGs and a label specifying if the patient was having a seizure during the recording or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading, preprocessing and checking the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the dataset\n",
    "urllib.request.urlretrieve('https://archive.ics.uci.edu/ml/machine-learning-databases/00388/data.csv', './data_ex3.csv')\n",
    "#read the csv file\n",
    "data_ex3 = pandas.read_csv('./data_ex3.csv')\n",
    "#modify labels since we are going to do a binary classification, seizure or no seizure, \n",
    "# and presence of seizure is represented by label 1\n",
    "data_ex3.y = data_ex3.y.map({1:1,2:0,3:0,4:0,5:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing the current table of the loaded dataset to see how it is organized\n",
    "data_ex3[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the data you just loaded\n",
    "train_data_ex3, val_data_ex3, test_data_ex3, train_labels_ex3, val_labels_ex3, test_labels_ex3 =  preprocess_medical_data(data_ex3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#sanity check of the result of normalization, one-hot encoding and shapes of the data vectors\n",
    "print('train_data_ex3:\\n ' + str(train_data_ex3))\n",
    "print('train_labels_ex3:\\n ' + str(train_labels_ex3))\n",
    "print('train_data_ex3.shape: ' + str(train_data_ex3.shape))\n",
    "print('train_labels_ex3.shape: ' + str(train_labels_ex3.shape))\n",
    "print('val_data_ex3.shape: ' + str(val_data_ex3.shape))\n",
    "print('val_labels_ex3.shape: ' + str(val_labels_ex3.shape))\n",
    "print('test_data_ex3.shape: ' + str(test_data_ex3.shape))\n",
    "print('test_labels_ex3.shape: ' + str(test_labels_ex3.shape))\n",
    "\n",
    "#checking how many of the labels are seizure labels\n",
    "#the dataset is unbalanced, but you should use it just like that\n",
    "print('Percentage of examples containing seizures: ' + str(np.sum(train_labels_ex3[:,1])/float(len(train_labels_ex3))*100) + '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a few functions that we will be using in this exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a softmax calculation with numerical stability tricks\n",
    "def softmax(logits, axis):\n",
    "    # subtracting the maximum logit from all logits for each example and prevents overflow \n",
    "    # of the exponential function of the logits and does not change results of the softmax\n",
    "    # because of properties of division of exponentials\n",
    "    stabilizing_logits = logits - np.expand_dims(np.max(logits, axis = axis), axis = axis)\n",
    "    \n",
    "    # clipping all logits to a minimum of -10 prevents underflow of the exponentials and \n",
    "    # only changes the result of the softmax minimally, since we know that one logit has value 0\n",
    "    # and exp^0>>exp(-10)\n",
    "    stabilizing_logits = np.clip(stabilizing_logits, -10, None)\n",
    "    \n",
    "    #using the softmax classic equation, but with the modified logits to prevent numerical errors\n",
    "    return np.exp(stabilizing_logits)/np.expand_dims(np.sum(np.exp(stabilizing_logits), axis = axis), axis = axis)\n",
    "\n",
    "# a forward function combined the two-layer network and the softmax\n",
    "def two_layer_network_softmax_forward(inputs, parameters):\n",
    "    logits = two_layer_network_forward(inputs, parameters)\n",
    "    return softmax(logits, axis = 1)\n",
    "\n",
    "# a forward function combined the two-layer network and the softmax\n",
    "def softmax_plus_ce_loss_backward(predicted, gt):\n",
    "    #the derivative of the output of softmax function followed by a cross-entropy loss\n",
    "    # with respect to the input is a beautifully simple equation equals to the softmax\n",
    "    # of the inputs minus the one-hot encoded groundtruth\n",
    "    return (softmax(predicted, axis = 1) - gt)\n",
    "\n",
    "#the calculation of the gradient for the classification network\n",
    "def two_layer_network_softmax_ce_backward(inputs, parameters, gt):\n",
    "    return two_layer_network_backward(inputs, parameters, gt, softmax_plus_ce_loss_backward)\n",
    "\n",
    "# a function to get how many logits predicted the right class when compared to gt\n",
    "def count_correct_predictions(logits, gt):\n",
    "    predicted_labels = one_hot(np.argmax(logits, axis = 1), logits.shape[1])\n",
    "    return np.sum(np.logical_and(predicted_labels,gt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C 3.1 (3 points)**: Write the body of the cross-entropy loss for a network output of any bidimensional shape and a one-hot encoded target of same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ce_loss(out, target):\n",
    "    ##your code starts here\n",
    "    \n",
    "    ##your code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test your function\n",
    "np.random.seed(1)\n",
    "ce_loss_testing_function = ce_loss(softmax(np.random.normal(0, 0.5, [12, 13]) + 3*one_hot(np.random.randint(13, size = 12), 13), axis = 1), one_hot(np.random.randint(13, size = 12), 13))\n",
    "if np.allclose(np.array(ce_loss_testing_function), np.array([3.1249936488336])):    \n",
    "    print('Your loss function seems to be returning the expected values')\n",
    "else:\n",
    "    print('Your loss function is not returning the expected values. You should review your code.')\n",
    "test_gradient(two_layer_network_softmax_forward, two_layer_network_softmax_ce_backward, ce_loss, train_data_ex3[0:10,:], train_labels_ex3[0:10,:], initialize_parameters_ex2(178, 10, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END C 3.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A 3.2 (13 points)**: Verify and analyze how the accuracy of a trained two-layer network changes as a function of the number of hidden nodes, for the provided dataset for this exercise. Use the provided learning rate, batch size and number of epochs. You should be able to get a validation accuracy higher than 96%. \n",
    "\n",
    "Hints:\n",
    "- Question 2.4 has code that may be useful here. In addition to what the code in question 2.4 is doing, you will need to loop over different numbers of hidden nodes to train different models, and also write inference loops to get the training and the validation accuracy of your model. For the accuracy calculation, you may find the provided `count_correct_predictions` function useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01 \n",
    "batch_size = 50\n",
    "n_epochs = 100\n",
    "\n",
    "##your code starts here\n",
    "\n",
    "##your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END A 3.2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C 3.3 (3 points)**: Test your model best model (model with best found hyperparameters) on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END C 3.3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: MNIST and weight decay\n",
    "**(Total of 24 points)**\n",
    "\n",
    "In this exercise, we are going to use a flattened and reduced MNIST dataset to train a two-layer neural network using L2 regularization. First, we load and preprocess the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ex4_train, x_ex4_val, x_ex4_test, y_ex4_train, y_ex4_val, y_ex4_test = load_and_preprocess_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sanity check to see that data is as it is supposed to be\n",
    "plt.imshow(x_ex4_train[1000,:].reshape(28,28), cmap = 'Greys')\n",
    "print(y_ex4_train[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C 4.1 (5 points)**: Considering an L2 penalty over all the weights parameters, and no penalty over bias parameters, complete the function below with the gradients of each different kind of parameter. The L2 penalty is defined as the sum of squares of every element of the penalized parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_regularization_backward(inputs, parameters, gt):\n",
    "    gradients = {}\n",
    "    for parameter_name in parameters.keys():\n",
    "        if 'weights' in parameter_name:\n",
    "            # complete the equation to calculate the l2 regularization loss gradient for weights\n",
    "            ##your code starts here\n",
    "            gradients[parameter_name] = \n",
    "            ##your code ends here\n",
    "        elif 'bias' in parameter_name:\n",
    "            # complete the equation to calculate the l2 regularization loss gradient for bias.\n",
    "            # Remember, the L2 regularization loss for bias is 0.\n",
    "            ##your code starts here\n",
    "            gradients[parameter_name] = \n",
    "            ##your code ends here\n",
    "    return gradients\n",
    "\n",
    "def two_layer_network_ce_and_l2_regularization_backward(inputs, parameters, gt, regularization_multiplier):\n",
    "    gradients = {}\n",
    "    gradients1 = two_layer_network_softmax_ce_backward(inputs, parameters, gt)\n",
    "    gradients2 = l2_regularization_backward(inputs, parameters, gt)\n",
    "    for parameter_name in parameters:\n",
    "        gradients[parameter_name] = gradients1[parameter_name] + regularization_multiplier * gradients2[parameter_name]\n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END C 4.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A 4.2 (16 points)**: Verify and analyze how the L2 regularization to weights changes the results for the provided dataset using a 200-hidden-nodes two-layer neural network. You should play a bit with the learning rate, batch size and number of epochs too, and be able to get more than 90% accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_hidden_nodes = 200\n",
    "\n",
    "##your code starts here\n",
    "\n",
    "##your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END A 4.2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C 4.3 (3 points)**: Test your best model using the provided test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END C 4.3**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dir",
   "language": "python",
   "name": "env_dir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
